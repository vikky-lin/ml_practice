{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM重要概念理解和公式推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">created by vikky  \n",
    ">2017-12-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于SVM的一些有趣的解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [svm解释](https://www.zhihu.com/question/21094489)  \n",
    "故事中红篮球的第一种分布是最简单的,即线性可分的数据集，简单的例子好入门。其中也涉及了许多重要的概念和专业名词，理解好这些有助于快速建立SVM的数学构造。先上图--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./picture/k_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 函数间隔VS几何间隔  \n",
    "  \n",
    "二者的关系与向量VS单位向量类似。  \n",
    "一般来说，点到超平面的距离远近可以表示分类预测的确信程度，什么意思呢？图中最左边的苹果相较于涂了颜色的苹果，你会有更大的把握相信前者是一个苹果，而后者有一定的可能性是被误分类的香蕉，这就是所谓的确信程度。  \n",
    "假设$X^{(i)}$表示第$i$个数据点的特征向量，$y^{(i)})$表示第$i$个数据点的分类标记，取值为-1或1。令图中的超平面为$W^T*X+B=0$,左边的数据集(苹果)分类为$1$，右边的数据集(香蕉)分类为$-1$,在超平面确定的情况下，$|W^T*X^{(i)}+B|$能够相对的表示一个数据点距离超平面的远近。而$|W^T*X^{(i)}+B|$的符号与分类标记符号$y^{(i)})$是否一致能够区分分类是否正确。所以可以用$d^{(i)}=y^{(i)}(W^T*X^{(i)}+B)$来表示数据点分类的正确性和确信程度。这就是函数间隔的概念。  \n",
    "根据这个概念可以看到，如果同时成比例的改变$W$和$B$，超平面不变，而函数间隔也会成比例改变。所以就有了几何间隔的概念--  \n",
    "与单位向量类似，将函数间隔$d^{(i)}$除以其法向量$W$就得到了几何间隔$d'$,即\n",
    "$$d'^{(i)}=\\frac {d^{(i)}}{||W||}= \\frac{y^{(i)}(W^T*X^{(i)}+B)}{||W||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 拉格朗日乘数法有什么用？\n",
    "  \n",
    "  先给出概念：拉格朗日乘数法就是将等式约束/不等式约束优化问题转化为无约束优化问题。  \n",
    "  课本给出的定义总是专业得让人头疼，先说说啥是约束，其实就是条件。举个栗子：  \n",
    "  1. 老板给你一个任务：让你自由发挥，建一座漂亮的房子。这时你可以想花多少钱就花多少钱，想怎么建就怎么建...这就是无约束的优化问题，姑且把漂亮当作优化的目标。\n",
    "  2. 这次老板提出了要求，说经费有限，只有100万，而且这100万还不能剩，让你也建一座漂亮的房子。这时你就得好好考虑怎么利用这100万，尽可能地建一座漂亮的房子出来...这就是等式约束优化问题。\n",
    "  3. 老板有提出了新的要求，说现在赶进度，让你在半年内把这房子建出来。老板只要求时间在半年内就行，你为了邀功当然也可以选择3个月就把房子就好...这就是不等式约束优化问题。  \n",
    "  \n",
    "有要求就会有约束，干起事来就会觉得束手束脚，拉格朗日乘数法就是将这些要求用某种形式与目标挂钩，让你在解决问题时也顺带着达到了这些要求。而不是先考虑要求再去解决问题，就是拉格朗日乘数法的作用，再详细点的介绍可以看看[这里](https://www.cnblogs.com/maybe2030/p/4946256.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 为什么要求解对偶问题  \n",
    "\n",
    "好了，通过拉格朗日乘数法我们把老板的要求融入到建漂亮房子这个最终要解决的问题中，一般来说这个演变后的无约束问题（称为原始问题）也挺好解决的。可有时也会碰到很棘手的要求，导致这个原始问题非常难解决，这时就可以通过在满足一定条件（KKT）的前提下，通过求解对偶问题来代替求解原始问题，使得原始问题求解更容易。这就是为什么要利用拉格朗日对偶性将原始问题转化为对偶问题的目的。  \n",
    "这个KKT条件也是关键，可以看下上面链接中的推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM构建目标函数过程推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性可分数据集中（参考上图），有无数个超平面可以将不同分类的数据集区分开来。而SVM的目标就是找到一个最优的超平面。怎么取定义最优？就是上面提到的几何间隔，让距离超平面最近的数据点（即所谓的支持向量）的几何间隔最大。设超平面$(W,B)$关于给定数据集的几何间隔为超平面关于数据集中所有样本点的几何间隔的最小值，即$D'=\\min d'^{(i)}$，则其数学描述为\n",
    "$$\\max_{W,B}D'$$  \n",
    "\n",
    "$$s.t.　\\frac{y^{(i)}(W^T*X^{(i)}+B)}{||W||}≥D'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据函数间隔与几何间隔的关系，上式可改写为\n",
    "$$\\max_{W,B}\\frac {d}{||W||}$$  \n",
    "\n",
    "$$s.t.　{y^{(i)}(W^T*X^{(i)}+B)}≥d$$  \n",
    "其中d表示支持向量到超平面的函数间隔。  \n",
    "\n",
    "根据前面的介绍，函数间隔$d$的取值不影响最优化问题的求解，为了简化问题，令$d=1$，代入得到\n",
    "$$\\max_{W,B}\\frac {1}{||W||}$$  \n",
    "\n",
    "$$s.t.　{y^{(i)}(W^T*X^{(i)}+B)}≥1$$  \n",
    "\n",
    "因为$||W||$取值肯定为正的（模），所以$\\frac {1}{||W||}$与$\\frac {1}{2}||W||^2$在相同点取到极值，所以最大化$\\frac {1}{||W||}$等价于最小化$\\frac {1}{2}||W||^2$，于是问题可以进一步优化为如下形式（注意约束条件的转换）\n",
    "$$\\min_{W,B}\\frac {1}{2}||W||^2$$  \n",
    "\n",
    "$$s.t.　1-{y^{(i)}(W^T*X^{(i)}+B)}≤0$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就得到了一个凸二次规划问题（我觉得暂时不用去理解含义），也是有不等式约束的优化问题。为此，要为每一个不等式约束引入拉格朗日乘子$\\alpha^{(i)}≥0，i$表示第$i$个不等式的$\\alpha$。接下来要构造拉格朗日函数\n",
    "① $$L(W,B,\\alpha)=\\frac {1}{2}||W||^2+\\sum_{i=1}^N \\alpha^{(i)}(1-{y^{(i)}(W^T*X^{(i)}+B)})$$ \n",
    "其中，$\\alpha=(\\alpha^{(1)},\\alpha^{(1)},\\cdot\\cdot\\cdot,\\alpha^{(n)})^T$为拉格朗日乘子向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于不等式约束条件很多，可以想象要直接求解难度很大，所以可以通过求解其对偶问题来求解原始问题。这里直接利用拉格朗日对偶性的定理，上面链接有推导过程。  \n",
    "根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：\n",
    "$$\\max_{\\alpha}\\min_{W,B}L(W,B,\\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 求解$\\min_{W,B}L(W,B,\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>对$L(W,B,\\alpha)$分别对$W$、$B$求偏导，并令其等于0\n",
    "$$\\bigtriangledown _W L(W,B,\\alpha)=W-\\sum_{i=1}^N \\alpha^{(i)}{y^{(i)}X^{(i)}}=0$$\n",
    "$$\\bigtriangledown _B L(W,B,\\alpha)=-\\sum_{i=1}^N \\alpha^{(i)}y^{(i)}=0$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">>得到\n",
    "$$W=\\sum_{i=1}^N \\alpha^{(i)}{y^{(i)}X^{(i)}}$$\n",
    "$$\\sum_{i=1}^N \\alpha^{(i)}y^{(i)}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>带入①中，得到\n",
    "$$\n",
    "\\begin {aligned}\n",
    "L(W,B,\\alpha)\\\\\\\n",
    "&=\\frac {1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})+\\\n",
    "\\sum_{i=1}^N \\alpha^{(i)}(1-{y^{(i)}(\\sum_{j=1}^N (\\alpha^{(j)}{y^{(j)}X^{(j)}})\\cdot X^{(i)}+B)})\\\\\\\n",
    "&=\\frac {1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})+\\\n",
    "\\sum_{i=1}^N \\alpha^{(i)}-\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})-\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}y^{(i)}B\\\\\\\n",
    "&=-\\frac {1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})+\\sum_{i=1}^N \\alpha^{(i)}\n",
    "\\end {aligned}\n",
    "$$   \n",
    "所以\n",
    "$$\\min_{W,B}L(W,B,\\alpha)=-\\frac {1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})+\\sum_{i=1}^N \\alpha^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* 求解$\\max_{\\alpha}\\min_{W,B}L(W,B,\\alpha)$，也就是求解原始问题的对偶问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$$\n",
    "\\begin {aligned}\n",
    "\\max_{\\alpha}\\min_{W,B}L(W,B,\\alpha)&=\\max_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})+\\sum_{i=1}^N \\alpha^{(i)}  \n",
    "\\end {aligned}\n",
    "$$\n",
    "$$s.t.　\\sum_{i=1}^N \\alpha^{(i)}y^{(i)}=0 $$\n",
    "$$\\alpha^{(i)}≥0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>等价于\n",
    "$$\n",
    "\\begin {aligned}\n",
    "\\min_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})-\\sum_{i=1}^N \\alpha^{(i)}  \n",
    "\\end {aligned}\n",
    "$$\n",
    "$$s.t.　\\sum_{i=1}^N \\alpha^{(i)}y^{(i)}=0 $$\n",
    "$$\\alpha^{(i)}≥0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是一堆的定理来证明原始问题有可行解，以及可以通过求对偶最优化问题的解$\\alpha$，并有$\\alpha$来求得原始问题（即最优超平面）的解$W$和$B$，下面直接给出结论\n",
    "$$\n",
    "W=\\sum_{i=1}^N \\alpha^{(i)}{y^{(i)}X^{(i)}}\n",
    "$$\n",
    "$$\n",
    "B=y^{(j)}-\\sum_{i=1}^N \\alpha^{(i)}y^{(j)}(X^{(i)}\\cdot X^{(j)})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
