{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **created by vikky**  \n",
    "> **2017-12-17**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***在下面的所有公式中大写字母表示向量***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 复合函数求偏导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial x}f(g(x))=f'(g(x))\\cdot g'(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在logistic回归中使用梯度下降法求损失函数的最优解时会求解三层复合函数的偏导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(W,X)=\\frac{1}{1+e^{-W^TX}}$$\n",
    "注意，logistic中$W$和$X$都不是实数，而是向量，而$W^T$表示W的转置矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对W求偏导$$\n",
    "\\begin {aligned}\n",
    "\\frac{\\partial}{\\partial W}f(W,X)&=\\frac{-1}{({1+e^{-W^TX})^2}}\\cdot \\frac{\\partial}{\\partial W}(1+e^{-W^TX}) \\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX})^2}}\\cdot e^{-W^TX}\\cdot \\frac{\\partial}{\\partial w}(-W^TX)\\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX})^2}}\\cdot e^{-W^TX}\\cdot (-X)\\\\\\\n",
    "&=\\frac{1}{({1+e^{-W^TX})^2}}\\cdot e^{-W^TX}\\cdot X\n",
    "\\end {aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 极大似然函数、梯度下降(上升)法的相关概念，自行百度，概念上很好理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic回归的基本过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **构造预测函数：暂时只考虑线性回归函数**  \n",
    "线性回归函数的描述形式为  \n",
    "$$f(X)=w_0+w_1x_1+w_2x_2+···+w_nx_n=W^TX\n",
    "$$\n",
    "在二分类问题中，会引入sigmoid函数，其数学描述形式为\n",
    "$$\n",
    "g(z)=\\frac{1}{1+e^{-z}}\n",
    "$$  \n",
    "将f(X)代入g(z),得到预测函数的基本形式\n",
    "$$\n",
    "h_W(X)=g(W^TX)=\\frac{1}{1+e^{-W^TX}}\n",
    "$$\n",
    "$h_W(X)$函数的含义是在给定参数W的情况下，输入一个特征向量X，其分类结果为1的概率。因为$h_W(X)\\in  (0,1)$，将阈值设为0.5，当概率大于0.5时，分类为1，反之为0。因此，对于输入的特征向量X分类结果为类别1和类别0的概率分别为：  \n",
    "$P(y=1|X;W)=h_W(X)$  \n",
    "$P(y=0|X;W)=1-h_W(X)$  \n",
    "数学描述形式为$$\n",
    "P(y^{(i)}|X^{(i)};W)=(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}\n",
    "$$\n",
    "其中$y^{(i)}$表示第i个样本的分类，取值为0或1，$X^{(i)}$表示第i个样本的特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **定义损失函数**，在这里即表示错误分类的概率  \n",
    "由预测函数$P(y^{(i)}|X^{(i)};W)$可知，在给定参数W的情况下，输入一个样本$X^{(i)}$，如果其类别被预测为1，概率为$h_W(X^{(i)})$，则这个样本被错误分类的概率为1-$h_W(X^{(i)})$；如果其类别被预测为0，概率为1-$h_W(X^{(i)})$，这个样本被错误分类的概率就为$h_W(X^{(i)})$；想要构造一个拟合度高的线性回归函数，就必须降低错误分类的概率，样本的特征向量X是无法预知的，所以想要降低错判概率，能调整的就是线性回归函数的参数W。这个错误分类的概率就是logistic回归二分类问题的损失函数，其数学描述形式为：\n",
    "$$J(W)=1-P(y^{(i)}|X^{(i)};W)=1-[(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度上升法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据以上描述，logistic的目标就是找到一组参数W($W_0,W_1,\\cdot\\cdot\\cdot ,W_n$),使得误判的概率降到最低。\n",
    "根据上面的损失函数J(W)可知，要想J(W)取到极小值，那么$(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}$,即$P(y^{(i)}|X^{(i)};W)$应取到极大值  \n",
    "假设有m个独立样本，那么其极大似然函数为$$l(W)=\\prod_{i=1}^{m}(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}$$  \n",
    "为方便计算，取对数$$\n",
    "\\begin {aligned}\n",
    "L(W)=\\log l(W)&=\\prod_{i=1}^{m}\\log(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}} \\\\\\\n",
    "&=\\sum_{i=1}^{m}[y^{(i)}\\log (h_W(X^{(i)}))+(1-y^{(i)})\\log (1-h_W(X^{(i)}))]\n",
    "\\end {aligned}\n",
    "$$  \n",
    "用梯度上升法描述L(W)的更新过程\n",
    "$$L(W)=L(W)+\\alpha \\cdot \\frac {\\partial}{\\partial W}L(W)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac {\\partial}{\\partial W}L(W)$的详细推导过程  \n",
    "这里是对W求偏导，所以$y^{(i)}$和$X^{(i)}$视为常数项  \n",
    "这里的$\\log$可以看成$\\ln$,求导后为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin {aligned}\n",
    "\\frac {\\partial}{\\partial W}L(W)&= \\frac {\\partial}{\\partial W}\\sum_{i=1}^{m}[y^{(i)}\\log (h_W(X^{(i)}))+(1-y^{(i)})\\log (1-h_W(X^{(i)}))]\\\\\\\n",
    "&=\\sum_{i=1}^{m}\\frac {y^{(i)}}{h_W(X^{(i)})}\\cdot \\frac {\\partial}{\\partial W}h_W(X^{(i)})+\\frac {1-y^{(i)}}{1-h_W(X^{(i)})}\\cdot \\\n",
    "\\frac {\\partial}{\\partial W}(1-h_W(X^{(i)})) \\\\\\\n",
    "&=\\sum_{i=1}^{m}\\frac {y^{(i)}}{h_W(X^{(i)})}\\cdot \\frac {\\partial}{\\partial W}h_W(X^{(i)})-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})}\\cdot \\\n",
    "\\frac {\\partial}{\\partial W}h_W(X^{(i)}) \\\\\\\n",
    "&=\\sum_{i=1}^{m}(\\frac {y^{(i)}}{h_W(X^{(i)})}-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})})\\cdot \\frac {\\partial}{\\partial W}h_W(X^{(i)})\n",
    "\\end {aligned}\n",
    "$$\n",
    "其中第3步到第4步为提取公因式，接下来对$\\frac {\\partial}{\\partial W}h_W(X^{(i)})$求偏导,\n",
    "根据预测函数的定义，公式中$$h_W(X^{(i)})=g(W^TX^{(i)})=\\frac{1}{1+e^{-W^TX^{(i)}}}$$则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin {aligned}\n",
    "\\frac {\\partial}{\\partial W}h_W(X^{(i)})&=\\frac {\\partial}{\\partial W}g(W^TX^{(i)})\\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX^{(i)}})^2}}·\\frac{\\partial}{\\partial W}(1+e^{-W^TX^{(i)}}) \\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX^{(i)}})^2}}·e^{-W^TX^{(i)}}·\\frac{\\partial}{\\partial w}(-W^TX^{(i)})\\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX^{(i)}})^2}}·e^{-W^TX^{(i)}}·(-X^{(i)})\\\\\\\n",
    "&=\\frac{1}{({1+e^{-W^TX^{(i)}})^2}}·e^{-W^TX^{(i)}}·X^{(i)}\\\\\\\n",
    "&=\\frac{1}{{1+e^{-W^TX^{(i)}}}}\\cdot \\frac{e^{-W^TX^{(i)}}·X^{(i)}}{{1+e^{-W^TX^{(i)}}}}·X^{(i)}\\\\\\\n",
    "&=h_W(X^{(i)})·(1-h_W(X^{(i)}))·X^{(i)}\n",
    "\\end {aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入原式，则\n",
    "$$\n",
    "\\begin {aligned}\n",
    "\\frac {\\partial}{\\partial W}L(W)&=\\sum_{i=1}^{m}(\\frac {y^{(i)}}{h_W(X^{(i)})}-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})})\\cdot \\\n",
    " \\frac {\\partial}{\\partial W}h_W(X^{(i)}) \\\\\\\n",
    "&=\\sum_{i=1}^{m}(\\frac {y^{(i)}}{h_W(X^{(i)})}-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})})\\cdot h_W(X^{(i)})·(1-h_W(X^{(i)}))·X^{(i)} \\\\\\\n",
    "&=\\sum_{i=1}^{m}(y^{(i)}\\cdot (1-h_W(X^{(i)}))-(1-y^{(i)})\\cdot h_W(X^{(i)}))\\cdot X^{(i)}\\\\\\\n",
    "&=\\sum_{i=1}^{m}(y^{(i)}-h_W(X^{(i)}))\\cdot X^{(i)}\n",
    "\\end {aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "终于推导完了，使用梯度上升法得到的线性回归函数的参数更新的等式为\n",
    "$$\n",
    "\\begin {aligned}\n",
    "L(W)&=L(W)+\\alpha \\cdot \\frac {\\partial}{\\partial W}L(W)\\\\\\\n",
    "&=L(W)+\\alpha \\cdot \\sum_{i=1}^{m}(y^{(i)}-h_W(X^{(i)}))\\cdot X^{(i)}\n",
    "\\end {aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在python代码中，参数$W$的更新代码为  \n",
    "$$weights = weights + alpha * dataMatrix.transpose() * error $$  \n",
    "error就是样本误判的概率，即$$error=y^{(i)}-h_W(X^{(i)})$$  \n",
    "这里的error不是一个值，而是一个矩阵。根据之前的假设，样本量为$m$，那么error为$m*1$的矩阵  \n",
    "代表的含义是在给定参数$W$的情况下，每个样本被误判的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未完待续..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
