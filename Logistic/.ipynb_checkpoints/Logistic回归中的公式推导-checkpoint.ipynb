{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **created by vikky**  \n",
    "> **2017-12-17**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***在下面的所有公式中大写字母表示向量***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 复合函数求偏导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial x}f(g(x))=f'(g(x))\\cdot g'(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在logistic回归中使用梯度下降法求损失函数的最优解时会求解三层复合函数的偏导，还涉及对向量求偏导，网上的推导好像都没提到这点，导致过程有点莫名其妙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(W,X)=\\frac{1}{1+e^{-W^TX}}$$\n",
    "注意，logistic中$W$和$X$都不是实数，而是向量，而$W^T$表示W的转置矩阵，给出$W$和$X$的具体形式：\n",
    "$$W=\\begin{bmatrix}\n",
    "\\\\ w_0 \n",
    "\\\\ w_1\n",
    "\\\\ \\cdot\n",
    "\\\\ \\cdot\n",
    "\\\\ w_n\\end\n",
    "{bmatrix},X=\\begin{bmatrix}\n",
    " x_1^{(1)}& x_1^{(2)} & \\cdot & \\cdot  & x_1^{(m)}\\\\ \n",
    " x_2^{(1)}& x_2^{(2)} & \\cdot & \\cdot  & x_2^{(m)}\\\\ \n",
    "\\cdot & \\cdot  & \\cdot & \\cdot  & \\cdot \\\\ \n",
    "\\cdot & \\cdot  & \\cdot & \\cdot  & \\cdot \\\\ \n",
    " x_n^{(1)}& x_n^{(2)} & \\cdot & \\cdot  & x_n^{(m)}\\\\ \n",
    "\\end{bmatrix}$$  \n",
    ">$x_2^{(3)}$表示第3个样本的第2个元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$是有n个元素，$X$对$W$的某个元素$W_j$求偏导$$\n",
    "\\begin {eqnarray}\n",
    "\\frac{\\partial}{\\partial W_j}f(W,X)&=&\\frac{-1}{({1+e^{-W^TX})^2}}\\cdot \\frac{\\partial}{\\partial W_j}(1+e^{-W^TX}) \\\\\\\n",
    "&=&\\frac{-1}{({1+e^{-W^TX})^2}}\\cdot e^{-W^TX}\\cdot \\frac{\\partial}{\\partial W_j}(-W^TX)\\\\\\\n",
    "&=&\\frac{-1}{({1+e^{-W^TX})^2}}\\cdot e^{-W^TX}\\cdot (-x_j)\\\\\\\n",
    "&=&\\frac{1}{({1+e^{-W^TX})^2}}\\cdot e^{-W^TX}\\cdot x_j\n",
    "\\end {eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">上式的推导过程中，$\\frac{\\partial}{\\partial W_j}(-W^TX)$涉及行向量对元素求导，结果为$\\frac{\\partial}{\\partial w_j}(-W^TX)=-x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 极大似然函数、梯度下降(上升)法的相关概念，自行百度，概念上很好理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic回归的基本过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **构造预测函数：暂时只考虑线性回归函数**  \n",
    "线性回归函数的描述形式为  \n",
    "$$f(X)=w_0+w_1x_1+w_2x_2+···+w_nx_n=W^TX\n",
    "$$\n",
    "在二分类问题中，会引入sigmoid函数，其数学描述形式为\n",
    "$$\n",
    "g(z)=\\frac{1}{1+e^{-z}}\n",
    "$$  \n",
    "将f(X)代入g(z),得到预测函数的基本形式\n",
    "$$\n",
    "h_W(X)=g(W^TX)=\\frac{1}{1+e^{-W^TX}}\n",
    "$$\n",
    "$h_W(X)$函数的含义是在给定参数W的情况下，输入一个特征向量X，其分类结果为1的概率。因为$h_W(X)\\in  (0,1)$，将阈值设为0.5，当概率大于0.5时，分类为1，反之为0。因此，对于输入的特征向量X分类结果为类别1和类别0的概率分别为：  \n",
    "$P(y=1|X;W)=h_W(X)$  \n",
    "$P(y=0|X;W)=1-h_W(X)$  \n",
    "数学描述形式为$$\n",
    "P(y^{(i)}|X^{(i)};W)=(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}\n",
    "$$\n",
    "其中$y^{(i)}$表示第i个样本的分类，取值为0或1，$X^{(i)}$表示第i个样本的特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **定义损失函数**，在这里即表示错误分类的概率  \n",
    "由预测函数$P(y^{(i)}|X^{(i)};W)$可知，在给定参数W的情况下，输入一个样本$X^{(i)}$，如果其类别被预测为1，概率为$h_W(X^{(i)})$，则这个样本被错误分类的概率为1-$h_W(X^{(i)})$；如果其类别被预测为0，概率为1-$h_W(X^{(i)})$，这个样本被错误分类的概率就为$h_W(X^{(i)})$；想要构造一个拟合度高的线性回归函数，就必须降低错误分类的概率，样本的特征向量$X$是无法预知的，所以想要降低错判概率，能调整的就是线性回归函数的参数$W$。这个错误分类的概率就是logistic回归二分类问题的损失函数，对单个样本来说，其损失函数的数学描述形式为：\n",
    "$$J_W(X^{(i)})=1-P(y^{(i)}|X^{(i)};W)=1-(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}$$  \n",
    "所以，当参数$W$确定后，整个样本集的损失函数为\n",
    "$$\n",
    "\\sum _{i=1}^{m}J_W(X^{i})=\\sum _{i=1}^{m}[1-P(y^{(i)}|X^{(i)};W)]=\\sum _{i=1}^{m}[1-(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度上升法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据以上描述，logistic的目标就是找到一组参数$W$($W_0,W_1,\\cdot\\cdot\\cdot ,W_n$),使得误判的概率降到最低。\n",
    "根据上面的损失函数$J_W(X^{(i)})$可知，要想$$\\sum _{i=1}^{m}J_W(X^{i})$$取到极小值，那么$$\\sum _{i=1}^{m}(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}$$应取到极大值。令$$l(W)=\\sum_{i=1}^{m}(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}}$$  \n",
    "$l(W)$并不是一个凸函数，所以需要取对数（详细解释见[这里](https://www.bilibili.com/video/av9912938/?from=search&seid=15242530760642572669#page=36)），令$$\n",
    "\\begin {aligned}\n",
    "L(W)=\\log l(W)&=\\sum_{i=1}^{m}\\log(h_W(X^{(i)}))^{y^{(i)}}(1-h_W(X^{(i)}))^{1-y^{(i)}} \\\\\\\n",
    "&=\\sum_{i=1}^{m}[y^{(i)}\\log (h_W(X^{(i)}))+(1-y^{(i)})\\log (1-h_W(X^{(i)}))]\n",
    "\\end {aligned}\n",
    "$$  \n",
    "用梯度上升法描述$W$的更新过程\n",
    "$$W^{i+1}=W^i+\\alpha \\cdot \\frac {\\partial}{\\partial W_j}L(W^i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac {\\partial}{\\partial W}L(W)$的详细推导过程  \n",
    "这里是对W求偏导，所以$y^{(i)}$和$X^{(i)}$视为常数项  \n",
    "这里的$\\log$可以看成$\\ln$,求导后为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin {aligned}\n",
    "\\frac {\\partial}{\\partial W}L(W)&= \\frac {\\partial}{\\partial W}\\sum_{i=1}^{m}[y^{(i)}\\log (h_W(X^{(i)}))+(1-y^{(i)})\\log (1-h_W(X^{(i)}))]\\\\\\\n",
    "&=\\sum_{i=1}^{m}\\frac {y^{(i)}}{h_W(X^{(i)})}\\cdot \\frac {\\partial}{\\partial W}h_W(X^{(i)})+\\frac {1-y^{(i)}}{1-h_W(X^{(i)})}\\cdot \\\n",
    "\\frac {\\partial}{\\partial W}(1-h_W(X^{(i)})) \\\\\\\n",
    "&=\\sum_{i=1}^{m}\\frac {y^{(i)}}{h_W(X^{(i)})}\\cdot \\frac {\\partial}{\\partial W}h_W(X^{(i)})-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})}\\cdot \\\n",
    "\\frac {\\partial}{\\partial W}h_W(X^{(i)}) \\\\\\\n",
    "&=\\sum_{i=1}^{m}(\\frac {y^{(i)}}{h_W(X^{(i)})}-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})})\\cdot \\frac {\\partial}{\\partial W}h_W(X^{(i)})\n",
    "\\end {aligned}\n",
    "$$\n",
    "其中第3步到第4步为提取公因式，接下来对$\\frac {\\partial}{\\partial W}h_W(X^{(i)})$求偏导,\n",
    "根据预测函数的定义，公式中$$h_W(X^{(i)})=g(W^TX^{(i)})=\\frac{1}{1+e^{-W^TX^{(i)}}}$$则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin {aligned}\n",
    "\\frac {\\partial}{\\partial W}h_W(X^{(i)})&=\\frac {\\partial}{\\partial W}g(W^TX^{(i)})\\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX^{(i)}})^2}}·\\frac{\\partial}{\\partial W}(1+e^{-W^TX^{(i)}}) \\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX^{(i)}})^2}}·e^{-W^TX^{(i)}}·\\frac{\\partial}{\\partial w}(-W^TX^{(i)})\\\\\\\n",
    "&=\\frac{-1}{({1+e^{-W^TX^{(i)}})^2}}·e^{-W^TX^{(i)}}·(-X^{(i)})\\\\\\\n",
    "&=\\frac{1}{({1+e^{-W^TX^{(i)}})^2}}·e^{-W^TX^{(i)}}·X^{(i)}\\\\\\\n",
    "&=\\frac{1}{{1+e^{-W^TX^{(i)}}}}\\cdot \\frac{e^{-W^TX^{(i)}}·X^{(i)}}{{1+e^{-W^TX^{(i)}}}}·X^{(i)}\\\\\\\n",
    "&=h_W(X^{(i)})·(1-h_W(X^{(i)}))·X^{(i)}\n",
    "\\end {aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入原式，则\n",
    "$$\n",
    "\\begin {aligned}\n",
    "\\frac {\\partial}{\\partial W}L(W)&=\\sum_{i=1}^{m}(\\frac {y^{(i)}}{h_W(X^{(i)})}-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})})\\cdot \\\n",
    " \\frac {\\partial}{\\partial W}h_W(X^{(i)}) \\\\\\\n",
    "&=\\sum_{i=1}^{m}(\\frac {y^{(i)}}{h_W(X^{(i)})}-\\frac {1-y^{(i)}}{1-h_W(X^{(i)})})\\cdot h_W(X^{(i)})·(1-h_W(X^{(i)}))·X^{(i)} \\\\\\\n",
    "&=\\sum_{i=1}^{m}(y^{(i)}\\cdot (1-h_W(X^{(i)}))-(1-y^{(i)})\\cdot h_W(X^{(i)}))\\cdot X^{(i)}\\\\\\\n",
    "&=\\sum_{i=1}^{m}(y^{(i)}-h_W(X^{(i)}))\\cdot X^{(i)}\n",
    "\\end {aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "终于推导完了，使用梯度上升法得到的线性回归函数的参数更新的等式为\n",
    "$$\n",
    "\\begin {aligned}\n",
    "L(W)&=L(W)+\\alpha \\cdot \\frac {\\partial}{\\partial W}L(W)\\\\\\\n",
    "&=L(W)+\\alpha \\cdot \\sum_{i=1}^{m}(y^{(i)}-h_W(X^{(i)}))\\cdot X^{(i)}\n",
    "\\end {aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在python代码中，参数$W$的更新代码为  \n",
    "$$weights = weights + alpha * dataMatrix.transpose() * error $$  \n",
    "error就是样本误判的概率，即$$error=y^{(i)}-h_W(X^{(i)})$$  \n",
    "这里的error不是一个值，而是一个矩阵。根据之前的假设，样本量为$m$，那么error为$m*1$的矩阵  \n",
    "代表的含义是在给定参数$W$的情况下，每个样本被误判的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未完待续..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关论文参考[这里](http://cs229.stanford.edu/notes/cs229-notes1.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
